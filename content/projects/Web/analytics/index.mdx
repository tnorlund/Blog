---
title: Analytics
slug: /projects/web/analytics
date: "2020-12-09T08:33:24"
description: "Trying to understand how people use this website on how to integrate the analysis vertically."
showToc: true
---
import MdxIcon from 'components/MdxIcon'
import Pandas from 'components/Icons/Pandas'

I decided to scale out the analytics I previously shared by diving deeper in more AWS services.

# First Attempt

From the last post, I was using Kinesis Firehose to store visitor data in an S3 bucket. An *Extract Transform Load*, ETL, pipeline is needed to store the data in DynamoDB.

![Glue](./Glue.svg)

I started by using Amazon Glue, a service provided by Amazon for ETL. I had a glue crawler move through the S3 bucket every 5 minutes to transform the raw data to manageable '.parquet' files.

![Parquet](./Parquet.svg)

Glue provides a table which is then used by Amazon EMR to distribute Spark clusters.

![Spark](./Spark.svg)

After running this for a week, it ended up costing me $10. This option is highly scalable, but costs a lot. I needed another option.

# Event-driven Functions

With previous Lambda experience, I thought that running a Lambda function every time Kinesis Firehose stored a '.parquet' file in the S3 bucket would be the best option.

My previous experience with Python came in handy when engineering the analytics data. I went with the Pandas library because of my past experience and Pandas's ability to parse '.parquet' files extremely fast.

<Pandas />

This method lets me remain "serverless" while keepings costs low.

# Database Architecture

The DynamoDB needed to be structured after developing the ETL pipeline. The first step in doing this was creating another ERD.

<MdxIcon filename={`analytics-ERD.svg`}/>

## Entities

Rather than going through [DeBrie's](https://twitter.com/alexbdebrie?s=21) steps for designing a DynamoDB table again, I'm just going to talk about the entities used to segment the data.

**Visitor**

The visitor is the center of all of the analytics produced. The visitor visits unique pages, uses a browser to request these pages, and is located in a certain place. I decided to use the visitor's access patterns to determine the primary key.

**Session**

While each visitor visits certain pages, they produce a "session." This session will determine their habits while accessing the different pages. 

The visitor, session, and other entities found in the ERD resulted in a table structure that required 2 global secondary indexes. The primary key is used to request the visitor-specific data.

|                  | PK                     | SK                            |
|------------------|------------------------|-------------------------------|
| Visit            | VISITOR#[IP Address]   | VISIT#[Date Visited]#[Slug]   |
| Visitor          | VISITOR#[IP Address]   | #VISITOR                      |
| Location         | VISITOR#[IP Address]   | #LOCATION                     |
| Browser          | VISITOR#[IP Address]   | #BROWSER#[Date Visited]       |
| Session          | VISITOR#[IP Address]   | SESSION#[Session Start Time]  |

The first global secondary index is used to request the page-specific data.

|                  | GSI1PK                 | GSI1SK                        |
|------------------|------------------------|-------------------------------|
| Visit            | PAGE#[Slug]            | VISIT#[Date Visited]          |
| Page             | PAGE#[Slug]            | #PAGE                         |
| Day              | PAGE#[Slug]            | #DAY#[Date]                   |
| Week             | PAGE#[Slug]            | #WEEK#[Date]                  |
| Month            | PAGE#[Slug]            | #MONTH#[Date]                 |
| Year             | PAGE#[Slug]            | #YEAR#[Date]                  |

Finally, the second global secondary index is used to request the session-specific data.

|                  | GSI2PK                 | GSI2SK                        |
|------------------|------------------------|-------------------------------|
| Visitor          | VISITOR#[IP Address]   | #VISITOR                      |
| Session          | VISITOR#[IP Address]   | SESSION#[Session Start Time]  |

# Transforming the Data

With the architecture of the table out of the way, the raw data needs to be processed to fit in the design. I ended up following this flow diagram in the Lambda function that runs whenever Kinesis Firehose stores a '.parquet' file in the S3 bucket. 

<MdxIcon filename={`AnalyticsFlow.svg`}/>

# Conclusion

This reduced costs over 90%, and allows me to still use EMR.

![EMR](./EMR.svg)

I understand why ElasticMap Reduce would still be needed. DynamoDB is mostly used for transacting data from a database, and I'm limited in how I can analyze the data after transforming the data through this single pipeline. With the raw data still in the data lake, I will always have the option to access the data using another method.

To keep costs at a minimum, an ETL pipeline needs to be implemented early in the product's life. With this event-based pipeline, I can still make realtime decisions based on users while paying a fraction of what most people are currently paying. 

From my experience, this is where most businesses and people fail to understand their users. It should not take days or weeks to create a report of how your users or customers use your product. They're most likely not using your product by then!

Today, companies must use analytics to understand their customers rather than asking them for feedback. A centralized place to store user analytics with realtime, event-driven business decisions is required to maintain competitiveness in a web-driven economy. The real question is: how much are you willing to pay to remain competitive? 