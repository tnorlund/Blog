---
title: Analytics
slug: /projects/web/analytics
date: "2020-12-09T08:33:24"
description: "Trying to understand how people use this website on how to integrate the analysis vertically."
showToc: true
---
import MdxIcon from 'components/MdxIcon'

I decided to scale out the analytics I previously shared by diving deeper in more AWS services.

# First Attempt

From the last post, I was using Kinesis Firehose to store visitor data in an S3 bucket. An *Extract Transform Load*, ETL, pipeline is needed to store the data in DynamoDB.

![Glue](./Glue.svg)

I started by using Amazon Glue, a service provided by Amazon for ETL. I had a glue crawler move through the S3 bucket every 5 minutes to transform the raw data to manageable '.parquet' files.

![Parquet](./Parquet.svg)

Glue provides a table which is then used by Amazon EMR to distribute Spark clusters.

![Spark](./Spark.svg)

After running this for a week, it ended up costing me $10. This option is highly scalable, but costs a lot. I needed another option.

# Event-driven Functions

With previous Lambda experience, I thought that running a Lambda function every time Kinesis Firehose stored a '.parquet' file in the S3 bucket would be the best option.

My previous experience with Python came in handy when engineering the analytics data. I went with the Pandas library because of my past experience and Pandas's ability to parse '.parquet' files extremely fast.

![Pandas](./Pandas.svg)

This method lets me remain "serverless" while keepings costs low.

# Database Architecture

The DynamoDB needed to be structured after developing the ETL pipeline. The first step in doing this was creating another ERD.

<MdxIcon filename={`analytics-ERD.svg`}/>

## Entities

Rather than going through [DeBrie's](https://twitter.com/alexbdebrie?s=21) steps for designing a DynamoDB table again, I'm just going to talk about the entities used to segment the data.

**Visitor**

The visitor is the center of all of the analytics produced. The visitor visits unique pages, uses a browser to request these pages, and is located in a certain place. I decided to use the visitor's access patterns to determine the primary key.

**Session**

While each visitor visits certain pages, they produce a "session." This session will determine their habits while accessing the different pages. 

The visitor, session, and other entities found in the ERD resulted in a table structure that required 2 global secondary indexes. The primary key is used to request the visitor-specific data.

|                  | PK                     | SK                            |
|------------------|------------------------|-------------------------------|
| Visit            | VISITOR#[IP Address]   | VISIT#[Date Visited]#[Slug]   |
| Visitor          | VISITOR#[IP Address]   | #VISITOR                      |
| Location         | VISITOR#[IP Address]   | #LOCATION                     |
| Browser          | VISITOR#[IP Address]   | #BROWSER#[Date Visited]       |
| Session          | VISITOR#[IP Address]   | SESSION#[Session Start Time]  |

The first global secondary index is used to request the page-specific data.

|                  | GSI1PK                 | GSI1SK                        |
|------------------|------------------------|-------------------------------|
| Visit            | PAGE#[Slug]            | VISIT#[Date Visited]          |
| Page             | PAGE#[Slug]            | #PAGE                         |
| Week             | PAGE#[Slug]            | #WEEK#[Date]                  |
| Month            | PAGE#[Slug]            | #MONTH#[Date]                 |

Finally, the second global secondary index is used to request the session-specific data.