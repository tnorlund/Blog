---
title: Adding A Back End
slug: /projects/web/adding-a-back-end
date: "2020-12-04T20:00:24"
description: "I added a way to login, follow projects, and comment on posts using Amplify."
showToc: true
---

import MdxIcon from 'components/MdxIcon'

After playing around with the Amplify command-line interface, I felt like I had the chops to add some more interactivity to this website. With a lot of coffee, and plenty of stack overflow, I was able to assemble something that's:
- scalable
- fast
- cheap

Amplify combines [Cognito](https://aws.amazon.com/cognito/), [API Gateway](https://aws.amazon.com/api-gateway/), and [DynamoDB](https://aws.amazon.com/dynamodb/), which allows developers to quickly ship a product. This was a great way to jump headfirst into the AWS ecosystem.

I've had experience using a few different relational databases, but DynamoDB is nothing close to what I've previously used. DynamoDB is a "key-value and document database that delivers single-digit millisecond performance at any scale." I decided to dive deeper into DynamoDB by picking up [Alex DeBrie's](https://twitter.com/alexbdebrie?s=21) book. After a lot of reading and testing, I was able to create a database that is easily accessible by the API created by Amplify.

# Signing In

Amplify uses Cognito for their authentication.

![Cognito](./AWSLogos_Cognito.svg)

Cognito handles the authentication and segments users based on the roles given to them. By using Cognito User Groups, I'm able to assign specific people 'Admin' access to the API. This allows me to change the website by using an API that only I and select users have permission to use.

# Creating the Database

Again, I've had no experience with DynamoDB. This book helped me tremendously, and after purchasing the book, you're invited to a group of other DynamoDB developers. 

![DynamoDB](./AWSLogos_DynamoDB.svg)

DeBrie has an organized system for creating a Dynamo DB table.

1. [Understand the application](#understanding-the-application)
2. [Create an ERD](#erd)
3. [Define the access patterns](#access-patterns)
4. [Design the primary keys and secondary indexes](#primary-key-and-indexes)

## Understanding the application

I wanted a way of communicating with users and allowing people to follow my work. I decided to have each blog post have its own comments while each project have its own followers.

## ERD

The entity relationship diagram describes how the data relates to one another. The one that I came up with shows how each user is related to the projects they follow and comments they've written.

<MdxIcon filename={`ERD.svg`}/>

## Access Patterns 

I know I needed an API when creating this back-end. Amplify creates both an API Gateway and a DynamoDB table when creating a representational state transfer, *REST*, API.

![API Gateway](./AWSLogos_APIGateway.svg)

I decided to separate the patterns into the 3 main entities seen in the ERD above. 

**User**
- Create a user
- Accept the terms of service
- Check to see if the latest terms of service has been accepted
- View followed projects per user
- View comments per user

**Projects**
- Follow a project
- Get all users that follow a project

**Post**
- Users can create, update, and delete their comment
- View comments and their replies per post

These patterns helped organize and structure the work required while developing this REST API.

## Primary Key and Indexes

DynamoDB uses a single table to organize all the data. This means that the relationships, sorting, and access patterns use a combination of the partition key, *PK*, and the sort key, *SK*, to access the data. Both the *PK* and *SK* are used to access the specific row requested from the table. 

Amplify also creates a Lambda function when it creates the API Gateway. This Lambda function is what contains the logic for each access pattern found in API Gateway.

![Lambda](./AWSLogos_Lambda.svg)

I found the best way to implement both the *PK* and *SK* was to create them while implementing the Lambda function.

### User

While DynamoDB needs unique identifiers to access each item, I decided to use a zero-padded number that uniquely describes the order of the users signing up for an account.

|                  | PK                 | SK                             |
|------------------|--------------------|--------------------------------|
| User             | USER#[User Number] | #USER                          |
| Terms Of Service | USER#[User Number] | #TOS#[Terms Of Service Number] |

The backend code is simplified by storing the entities in the `/entities` folder and the data accesses patterns in the `/data` folder. Here's an example of the user entity.

```js:title=entities/user.js
class User {
  constructor( { 
    name, email, userNumber = `0`, 
    dateJoined = new Date(), numberTOS = `0` 
  } ) {
    if ( !name )
      throw Error( `Must give the user's name` )
    this.name = name
    if ( !email )
      throw Error( `Must give the user's email` )
    this.email = email
    this.userNumber = parseInt( userNumber )
    this.dateJoined = dateJoined
    this.numberTOS = parseInt( numberTOS )
    this.numberFollows = parseInt( numberFollows )
  }

  pk() {
    return { 'S': `USER#${
      ( `00000` + this.userNumber ).slice( -6 )
    }` }
  }

  key() {
    return {
      'PK': { 'S': `USER#${
        ( `00000` + this.userNumber ).slice( -6 )
      }` },
      'SK': { 'S': `#USER` }
    }
  }

  toItem() {
    return {
      ...this.key(),
      'Type': { 'S': `user` },
      'Name': { 'S': this.name },
      'Email': { 'S': this.email },
      'DateJoined': { 'S': this.dateJoined.toISOString() },
      'NumberTOS': { 'N': this.numberTOS.toString() },
      'NumberFollows': { 'N': this.numberFollows.toString() }
    }
  }
}

const userFromItem = ( item ) => {
  return new User( {
    name: item.Name.S,
    email: item.Email.S,
    userNumber: item.PK.S.split( `#` )[1],
    dateJoined: item.DateJoined.S,
    numberTOS: item.NumberTOS.N
  } )
}

module.exports = { User, userFromItem }
```

A Terms of Service agreement provides me some legal liability when it comes to storing people's information. The agreements must be accessible by the user's details:

```js:title=data/getUserDetails.js
const getUserDetails = async ( tableName, user ) => {
  if ( !tableName ) throw Error(
    `Must give the name of the DynamoDB table`
  )
  try {
    const result = await dynamoDB.query( {
      TableName: tableName,
      KeyConditionExpression: `#pk = :pk`,
      ExpressionAttributeNames: { '#pk': `PK` },
      ExpressionAttributeValues: { ':pk': user.pk() },
      ScanIndexForward: false
    } ).promise()
    if ( !result.Items )
      return { error: `User does not exist` }
    let requestedUser
    let tos = []
    let follows = []
    let comments = []
    let votes = []
    result.Items.map( ( item ) => {
      switch ( item.Type.S ) {
        case `user`:
          requestedUser = userFromItem( item )
          break
        case `terms of service`:
          tos.push( tosFromItem( item ) )
          break
        case `project follow`:
          follows.push( projectFollowFromItem( item ) )
          break
        case `comment`:
          comments.push( commentFromItem( item ) )
          break
        case `vote`:
          votes.push( voteFromItem( item ) )
          break
        default:
          throw Error(
            `Could not parse type ${ item.Type.S }`
          )
      }
    } )
    return {
      user: requestedUser, votes, tos, comments, follows
    }
  } catch( error ) {
    console.log( `ERROR getUserDetails`, error )
    return { error: `Could not get user` }
  }
}
```

Now the user and their details are accessible through the client. You can create an account by clicking the person in the top right corner.

### Project

DynamoDB can be indexed by multiple indexes. Each project's unique follow uses a Global Secondary Index that allows DynamoDB to access the items by both the user's unique number and the project's slug.

|                  | PK                     | SK                            |
|------------------|------------------------|-------------------------------|
| Project          | #PROJECT               | PROJECT#[Project Slug]        |
| Project Follow   | USER#[User Number]     | #PROJECT#[User Follow Number] |

The user can request their details by querying a partition key, *PK*, `USER#[User Number]` while the project can request its data with a partition key, *GSI1PK*, `PROJECT#[Project Slug]`.

|                  | GSI1PK                 | GSI1SK                           |
|------------------|------------------------|----------------------------------|
| Project          | PROJECT#[Project Slug] | PROJECT#[Project Slug]           |
| Project Follow   | PROJECT#[Project Slug] | #PROJECT#[Date Followed]         |

These different indexing methods allow DynamoDB to remain simple and efficient.

### Comments

I used the previous two techniques to model the comments made by users.

|         | PK                 | SK                    |
|---------|--------------------|-----------------------|
| Post    | POST#[Post Slug]   | #POST                 |
| Comment | USER#[User Number] | #COMMENT#[Date Added] |
| Vote    | USER#[User Number] | #VOTE#[Date Added]    |

The Global Secondary Index is used to query the all of the comments per post.

|         | GSI1PK           | GSI1SK                                          |
|---------|------------------|-------------------------------------------------|
| Post    | POST#[Post Slug] | #POST                                           |
| Comment | POST#[Post Slug] | #COMMENT#[Date Added]                           |
| Vote    | POST#[Post Slug] | #COMMENT#[Comment Date Adeed]#VOTE#[Date Added] |

Looking back at the [ERD](#erd), the comments can also be replies of other comments. This requires advanced indexing techniques when it comes to accessing the details of the post.

```js:title=data/getPostDetails.js
const getPostDetails = async ( tableName, post ) => {
  if ( !tableName ) throw Error(
    `Must give the name of the DynamoDB table`
  )
  try {
    const result = await dynamoDB.query( {
      TableName: tableName,
      IndexName: `GSI1`,
      KeyConditionExpression: `#gsi1pk = :gsi1pk`,
      ExpressionAttributeNames: { '#gsi1pk': `GSI1PK` },
      ExpressionAttributeValues: {
        ':gsi1pk': post.gsi1pk()
      },
      ScanIndexForward: true
    } ).promise()
    if ( !result.Items )
      return { error: `Post does not exist` }
    let comments = {}
    let requestedPost
    result.Items.map( ( item ) => {
      switch ( item.Type.S ) {
        case `post`:
          requestedPost = postFromItem( item ); break
        case `comment`: {
          const requestedComment = commentFromItem( item )
          comments = addCommentToComments(
            requestedComment, comments,
            [...requestedComment.replyChain]
          )
          break
        }
        case `vote`: {
          const requestedVote = voteFromItem( item )
          comments = addVoteToComments(
            requestedVote, comments,
            [...requestedVote.replyChain]
          )
          break
        }
        default: throw Error(
          `Could not parse type ${ item.Type.S }`
        )
      }
    } )
    return { post: requestedPost, comments }
  } catch( error ) {
    console.log( `ERROR getPostDetails`, error )
    return { error: `Could not get project details` }
  }
}
```

Both the votes and replies leverage recursion due to DynamoDB querying sorted-data (`replyChain` parameter are the date-times that construct the *GSI1SK*).

```js:title=data/getPostDetails.js
const addVoteToComments = (
  vote, comments, replyChain
) => {
  const date = replyChain.shift().toISOString()
  if (
    Object.keys( comments ).indexOf( date ) >= 0 &&
    replyChain.length == 0
  ) {
    comments[ date ]
      .votes[vote.dateAdded.toISOString()] = vote
    return comments
  } else {
    comments[ date ].replies = addVoteToComments(
      vote, comments[ date ].replies, replyChain
    )
    return comments
  }
}
```

All of these access patterns are efficient and do not require a server to be constantly running. This allows me to only pay *per millisecond* of accessing the data.

# Analytics

After adding all of this, I'd like to understand how people interact with it. Amplify makes analytics really easy by integrating Kinesis.

![Kinesis](./AWSLogos_Kinesis.svg)

Kinesis Data Streams and Kinesis Data Firehose are used to record the visitor's IP address into an S3 Bucket.

![S3](./AWSLogos_S3.svg)

The S3 bucket acts as a data-lake for storing raw data. AWS glue is used to process the data into a parquet file every 5 minutes. I'm using [ipify](https://geo.ipify.org) to obtain location and proxy information from the raw IP address. This transformed data is then stored in another DynamoDB table.

This might be controversial, but I don't want to use Google Analytics. I had the option to use a database that uses Online Analytic Processing, *OLAP*, but I prefer to keep costs as low as possible by continuing to leverage serverless technologies. I'm continuing to use DynamoDB to store the analytics data obtained from this website.

# Conclusion

This was a lot of work. I know that I had the option to use [Disqus](https://disqus.com), but it's \$100 a year. This costs maybe \$0.10 a month, and it can scale to hundreds of dollars a year.

I was able to pick up a large portion of the services AWS has to offer while also learning how to optimize and construct DynamoDB tables. Once I have some insight in how people use these services, I'll be sure to share ASAP.